{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf6b200d",
   "metadata": {},
   "source": [
    "## Regressão Linear através de estatística\n",
    "\n",
    "$$ X \\implies Y $$\n",
    "\n",
    "Y é dependente de X, por isso essas variáveis são chamadas de:\n",
    "- Variável resposta ou dependente : Y\n",
    "- Variável preditora ou independente: X\n",
    "\n",
    "O objetivo da regressão linear é entender como X explica Y. Assim buscamos encontrar o estimador função de regressão $f(x)$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "& f(x) = E[Y | X = x] = \\theta_0 + \\theta_1x\n",
    "\\\\\n",
    "& \\footnotesize\\text{Repare que é equivalente a equação da reta.}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ce05f",
   "metadata": {},
   "source": [
    "## Regressão Linear Simples\n",
    "\n",
    "Podemos considerar um modelo de regressão linear onde:\n",
    "\n",
    "$$\n",
    "Y_i = θ_0 + θ_1x_i + ϵ_i\n",
    "$$\n",
    "\n",
    "Onde assumimos que $ϵ_i ∼ \\mathcal{N}(0,σ^2)$  (ou seja, os resíduos são derivados de distribuições normais,*toda a aleatoriedade do modelo vem de $ϵ_i$*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcc2db0",
   "metadata": {},
   "source": [
    "Dessa forma, a média de $Y_i$ será sempre sua posição na curva. Logo, Y pode ser explicado como sendo uma distribuição normal com os seguintes parâmetros:\n",
    "$$ Y_i ∼ \\cal{N}(θ_0 + θ_1x_i, σ^2) $$\n",
    "\n",
    "*Prova* - Como a esperança de uma normal é μ:\n",
    "$$\\begin{align*}\n",
    "\n",
    "E[Y_i] &= μ = E[μ] \\\\\n",
    "&= E[θ_0 + θ_1x_i + ϵ_i] \\\\\n",
    "&= E[θ_0] + E[θ_1x_i] + E[ϵ_i] \\\\\n",
    "∴ E[Y_i]&= θ_0 + θ_1x_i + 0 \\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Assim, através da função de probabilidade da dist. normal:\n",
    "\n",
    "$$\n",
    "f_{Y_i | x_i}(y_i|x_i;θ_0, θ_1) = \\frac {1} {σ \\sqrt{2π}} \\exp({-\\frac{1}{2σ^2}(y_i-(θ_0+θ_1x_i))^2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4a393c",
   "metadata": {},
   "source": [
    "Sabendo que a função de verossimilhança calcula a probabilidade de se observar os dados $y$ dado um $θ$, vamos calcular os parâmetros usando o método da máxima verossimilhança.\n",
    "\n",
    "$$ \\begin{gather*}\n",
    "L(θ_0, θ_1; y, x) = \\prod_{i=1}^{n} f_{Y_i | x_i}(y_i|x_i;θ_0, θ_1) = \\prod_{i=1}^{n} \\frac {1} {σ \\sqrt{2π}} \\exp({-\\frac{1}{2σ^2}(y_i-(θ_0+θ_1x_i))^2}) \\\\\n",
    "\\argmax_{θ_0, θ_1}\\ln ∘ {L(θ_0, θ_1 ; y, x)} = \\hat{θ}^T =[\\hat{θ_0}, \\hat{θ_1}] \\\\\\\\\n",
    "\\end{gather*} \\\\\n",
    "\n",
    "\\begin{align*}\n",
    "\\ln ∘ {L(θ_0, θ_1 ; y, x)} &= \\sum_{i=1}^{n} \\left( \\ln(\\frac {1} {σ \\sqrt{2π}}) -\\frac{1}{2σ^2}(y_i-(θ_0+θ_1x_i))^2 \\right) \\\\\n",
    "&= \\sum_{i=1}^n \\left( -\\frac{n}{2}\\ln(2πσ^2) - \\frac{1}{2σ^2} (θ_0 + θ_1x_i - y_i)^2  \\right)\\\\\n",
    "&= -\\frac{n}{2}\\ln(2πσ^2) - \\frac{1}{2σ^2} \\sum_{i=1}^n (θ_0 + θ_1x_i - y_i)^2 \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965866ee",
   "metadata": {},
   "source": [
    "Para maximizar os parâmetros, basta então derivar o ln da função de verossimilhança em relação a $θ_0$ e $θ_1$ e igualar a zero.\n",
    "Os termos constantes virarão 0 na derivada, sobrando apenas a soma dos quadrados dos resíduos. O problema de maximização então se torna um problema de minimização pois há o fator negativo.\n",
    "\n",
    "Dessa forma, cairá na mesma formulação do método dos mínimos quadrados que foi resolvida no arquivo anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb60ae1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Intervalo de confiança\n",
    "\n",
    "Já que $\\hat{θ}$ é um estimador, podemos construir intervalos de confiança para os parâmetros estimados.\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "IC(θ_0; 1 - α) = \\hat{θ_j} ± t_{\\frac{α}{2}, n-2} \\cdot \\frac {S_ϵ \\sqrt{\\sum_{i=1}^{n} x_i^2}} {\\sqrt{n\\sum_{i=1}^n(x_i-\\bar{x})^2}} \\\\\n",
    "\n",
    "IC(θ_1; 1 - α) = \\hat{θ_j} ± t_{\\frac{α}{2}, n-2} \\cdot \\frac {S_ϵ} {\\sqrt{\\sum_{i=1}^n(x_i-\\bar{x})^2}}\n",
    "\\end{gather}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36848c04",
   "metadata": {},
   "source": [
    "### Teste de hipótese\n",
    "Podemos também fazer certos testes de hipótese em relação aos dois parâmetros:\n",
    "\n",
    "#### Coeficiente Linear\n",
    "$$\n",
    "H_0 : θ_0 = 0 \\\\\n",
    "H_1 : θ_0 ≠ 0\n",
    "$$\n",
    "\n",
    "A hipótese nula diz que a reta passa pela origem.\n",
    "\n",
    "#### Coeficiente Angular\n",
    "$$\n",
    "H_0 : θ_1 = 0 \\\\\n",
    "H_1 : θ_1 ≠ 0\n",
    "$$\n",
    "\n",
    "A hipótese nula diz a reta é horizontal, ou seja, não há relação entre X e Y.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
